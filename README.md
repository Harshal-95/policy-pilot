# PolicyPilot  
### Enterprise AI-Powered Policy Intelligence System  

PolicyPilot is a full-stack AI application that enables intelligent querying and analysis of policy documents using **Retrieval-Augmented Generation (RAG)**.

It allows users to upload policy PDFs and ask contextual questions. The system retrieves relevant document sections using semantic vector search and generates grounded, enterprise-grade responses using a local LLM.

---

## ğŸš€ Core Idea

Policy documents are often long, complex, and difficult to navigate.

PolicyPilot solves this problem by:

- Converting policy PDFs into structured text chunks  
- Generating embeddings for semantic understanding  
- Storing embeddings in a vector database  
- Retrieving top relevant sections for a query  
- Generating factual answers using a local LLM  
- Enforcing a strict **no-hallucination, context-only policy**

---

## ğŸ—ï¸ Architecture Overview

User
â†“
React Frontend (Vite)
â†“
Flask Backend API
â†“
Document Ingestion & Chunking
â†“
Sentence Transformers (Embeddings)
â†“
Pinecone Vector Database
â†“
Top-K Retrieval
â†“
Ollama Runtime + Mistral 7B
â†“
Enterprise-Grade Response


This system follows a **retrieval-first architecture** to ensure responses are grounded strictly in uploaded document content.

---

## ğŸ§  Tech Stack

### ğŸ”¹ Frontend
- React (Vite)
- Modern SaaS-style Dashboard UI
- Dark / Light Mode
- Multi-conversation Chat System
- AI Thinking Animation

### ğŸ”¹ Backend
- Flask (Python)
- REST APIs
- Flask-CORS
- Python-Dotenv

### ğŸ”¹ AI / RAG Stack
- Sentence Transformers (`all-MiniLM-L6-v2`)
- Pinecone (Vector Database)
- Ollama (Local LLM Runtime)
- Mistral 7B (LLM Model)
- Custom Prompt Engineering

### ğŸ”¹ Databases
- PostgreSQL (Metadata & Conversation Storage â€“ extendable)
- Pinecone (Semantic Vector Search)

---

## âš™ï¸ How It Works

### 1ï¸âƒ£ Document Upload
- User uploads a policy PDF  
- Backend extracts text and tables  
- Content is split into structured chunks  
- Metadata (e.g., page numbers) is preserved  

### 2ï¸âƒ£ Chunking & Embedding
- Each chunk is converted into a vector embedding  
- Stored in Pinecone for semantic similarity search  

### 3ï¸âƒ£ Question Answering
- User question is embedded  
- Pinecone retrieves top-K relevant chunks  
- Retrieved context is passed to Mistral (via Ollama)  
- LLM generates a grounded response  

### 4ï¸âƒ£ Safe Enterprise Responses
- Answers strictly based on retrieved context  
- No hallucination or external assumptions  
- If information is missing, system responds professionally  

## ğŸ’¬ Conversation System
 
- Dynamic conversation switching  
- Auto-generated conversation titles  
- In-memory session storage (extendable to PostgreSQL)  
- Recent conversations sidebar  

---

## ğŸ¨ UI Features

- Professional dashboard layout  
- Scrollable sidebar  
- Dynamic document list  
- Multi-session chat  
- Auto-scroll to latest message  
- Dark / Light theme toggle  

---

## ğŸ“‚ Project Structure

policy-pilot/
â”‚
â”œâ”€â”€ Policy Clarification Model/ # Flask Backend
â”‚ â”œâ”€â”€ app.py
â”‚ â”œâ”€â”€ ingestion/
â”‚ â”œâ”€â”€ rag/
â”‚ â”œâ”€â”€ db/
â”‚ â”œâ”€â”€ requirements.txt
â”‚ â””â”€â”€ .env (not committed)
â”‚
â”œâ”€â”€ policy-ui/ # React Frontend
â”‚ â”œâ”€â”€ components/
â”‚ â”œâ”€â”€ layout/
â”‚ â””â”€â”€ styles/
â”‚
â””â”€â”€ .gitignore


---

## ğŸ› ï¸ Setup Instructions

### ğŸ”¹ Backend Setup

```bash
python -m venv venv
venv\Scripts\activate
pip install -r requirements.txt
Create a .env file:

PINECONE_API_KEY=your_key_here
PINECONE_INDEX=policy-index
Run backend:

python app.py
ğŸ”¹ Frontend Setup
cd policy-ui
npm install
npm run dev
ğŸ”¹ Ollama + Mistral Setup
Install Ollama:
https://ollama.com

Pull model:

ollama pull mistral
Run model:

ollama serve
ğŸ§­ Design Principles
Retrieval-first architecture

Zero hallucination policy

Context-only answering

Modular & scalable backend design

Enterprise-grade response tone

ğŸ¯ Ideal Use Cases
Insurance companies

Legal & compliance teams

Corporate policy management
